{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter notebook --NotebookApp.max_buffer_size=6000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "#Se importan las bibliotecas. En caso de querer hacer este proceso directamente\n",
    "#en un ordenador se deben instalar las bilbiotecas que se indican a continuacion:\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py\n",
    "import sklearn\n",
    "import glob\n",
    "import keras\n",
    "import time\n",
    "import imp\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#Estas bibliotecas no son necesarias en su totalidad, solo se requieren ciertas\n",
    "#funciones\n",
    "from random import shuffle\n",
    "from skimage import io\n",
    "from collections import Counter\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#Importe de funciones especificas de bibliotecas ya importadas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import BatchNormalization, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "#lineas requeridas solo en jupyter notbook\n",
    "%matplotlib inline\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se inicia definiendo el diccionario map_characters, este posee originalmente 18\n",
    "#personajes, si se agrega otro más se debe agregar al diccionario y colocarsele \n",
    "#el número consecutivo que corresponda\n",
    "map_characters = {0: 'trainingCoffee'}\n",
    "                  \n",
    "#Se define el numero de clases con la extensión del diccionario\n",
    "num_classes = len(map_characters)\n",
    "\n",
    "#Número máximo de imagenes que se usaran por personaje para entrenar\n",
    "pictures_per_class = 12000\n",
    "\n",
    "def load_pictures(test_size, pic_size,BGR):\n",
    "    \"\"\"\n",
    "    Load pictures from folders for characters from the map_characters dict and create a numpy dataset and \n",
    "    a numpy labels set. Pictures are re-sized into picture_size square.\n",
    "    :param BGR: boolean to use true color for the picture (RGB instead of BGR for plt)\n",
    "    :return: dataset, labels set\n",
    "    \"\"\"\n",
    "    pics = []    #Se crea la lista de imagenes de salida\n",
    "    labels = []  #Se crea la lista de etiquetas de salida\n",
    "\n",
    "    #k es el numero de la variable en el diccionario\n",
    "    #char es el nombre de la variable en el diccionario\n",
    "    # El if lo que hace es que recorra cada una de las variables del diccionario\n",
    "    for k, char in map_characters.items(): \n",
    "\n",
    "        #Esta linea deivuelve la lista de rutas de las imagenes de cada personaje del diccionario\n",
    "        # Ej: para char = 'abraham_grampa_simpson' regresa todas las rutas de imagen de la carpeta\n",
    "        # abraham_grampa_simpson que a su vez esta en la carpeta characters.\n",
    "        #Es solo una forma elegante y rapida de optener todas las rutas\n",
    "        pictures = [k for k in glob.glob('./source/trainingSet/%s/*' % char)]\n",
    "\n",
    "        #Se establece la cantidad de imagenes que se van a usar en cada clase\n",
    "        #Como maximo se usan 1176 imagenes, estas son tanto para train como para test\n",
    "        #Si el personaje en particular tiene menos de 1176 imagenes se usara la extension del personaje en cuestion\n",
    "        nb_pic = round(pictures_per_class/(1-test_size)) if round(pictures_per_class/(1-test_size))<len(pictures) else len(pictures)\n",
    "        \n",
    "        #Se usa la funcion random para ordenar de manera aleatoria las imagenes de cada personaje\n",
    "        #Cabe detacar que pic es una ruta de acceso aleatoria de la lista de rutas de acceso en pictures\n",
    "        for pic in np.random.choice(pictures, nb_pic):\n",
    "\n",
    "            #Se lee la imagen de la ruta \"pic\"\n",
    "            a = cv2.imread(pic)\n",
    "            # a = cv2.imread(pic, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            #Si se desea se usa RGB se hace la transformacion de BGR, que es la lectura original\n",
    "            #Por defecto este cambio no se hace a menos que se especifique\n",
    "            if BGR:\n",
    "                a = cv2.cvtColor(a, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            #Se hace un resize de las imagenes a pic_size x pic_size. Por defecoto\n",
    "            #este es 64x64. Esto concuerda con los pixeles de entrada de la red neuronal\n",
    "            a = cv2.resize(a, (pic_size,pic_size))\n",
    "\n",
    "            #Se agrega la imagen leida, convertida y escalada a la lista pics\n",
    "            pics.append(a)\n",
    "            #Se agrega la etiqueta numérica de la lista de etiquetas, esta etiqueta numerica \n",
    "            #concuerda con el número en el diccionario map_characters\n",
    "            labels.append(k)\n",
    "\n",
    "    #Se devuelve las listas pero antes se transforman a un formato de array con numpy\n",
    "    return np.array(pics), np.array(labels) \n",
    "\n",
    "def get_dataset(save=False, load=False, BGR=False, test_size=0.1, pic_size=64):\n",
    "    #Primero se llama a la funcion load_pictures, por defecto se deja estas en \n",
    "    #blanco y negro\n",
    "    #En esta funcion tambien se extraen los labels del archivo txt anotations\n",
    "    X, y = load_pictures(test_size, pic_size, BGR)\n",
    "\n",
    "    #Luego se pasa los labels a un formato numerico y en listas separadas,\n",
    "    #por ejemplo si el label es 2 to_categorical lo pasa a una lista [0 0 1 ... 0]\n",
    "    #El tamaño de la lista depende de cuantas clases se tenga, en este ejemplo son 18\n",
    "    y = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "    #Se normaliza las imagenes para que los valores de cada banda esten en punto flotante\n",
    "    #y además se encuentren entre 0 y 1. Esto ayuda a la velocidad de entrenamiento           \n",
    "    X = X.astype('float32') / 255.\n",
    "    \n",
    "    #Se imprime las formas de cada tensor. Si se mantienen todas las variables \n",
    "    #por defecto estas deben ser (x,64,64,3) y (x,w), donde x es el numero de imagenes\n",
    "    #y w es la cantidad de clases en el diccionario\n",
    "    print(\"Train\", X.shape, y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "#Uso de funciones de lectura para dividir el set de imagnes en test y train\n",
    "X, y = get_dataset(save=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "axs.imshow(X[6])\n",
    "plt.show()\n",
    "axs.axis('off')\n",
    "fig.savefig(\"images2.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 64\n",
    "img_cols = 64\n",
    "channels = 3\n",
    "latent_dim = 100\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256 * 16 * 16, input_dim=100))\n",
    "    model.add(Reshape((16, 16, 256)))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    #Se crea primeramente un modelo \"Sequential\", este es el tipo de red neuronal\n",
    "    #existen otros tipos como las recursivas y las recurrentes.\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #Se usa mucho las capas Conv2D, por lo que se explicara solo una vez. Estas \n",
    "    #tienen el formato por defecto:\n",
    "    \n",
    "    #tf.keras.layers.Conv2D(\n",
    "    #    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    #    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "    #    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    #    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    #    kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "    #\n",
    "    #Donde:\n",
    "    #\"filters\" se refiere a la cantidad de filtros que se tienen en esa capa, \n",
    "    #por ejemplo en la primera capa se tiene 32 filtros.\n",
    "    #Los filtros son los que cambuan durante el entrenamiento.\n",
    "    \n",
    "    #\"strides\" es siempre una tupla. Se refiere a las dimenciones dadas para \n",
    "    #cada filtro, por defecto esta variable tiene como valor la tupla (1,1). \n",
    "    #Por ejemplo, en la primera capa se usa una dimensión para los filtros (3,3)\n",
    "    \n",
    "    #\"padding\" es una bandera que puede ser \"valid\" (opcion por defecto) \n",
    "    #o \"same\" (case-insensitive). La opción valid tiene la posibilidad de perder\n",
    "    #informacion de entrada en especial de las esquinas. Por otra parte same\n",
    "    #aplica rellenos en las esquinas de modo que se usen todos los pixeles de entrada\n",
    "\n",
    "    #\"input_shape\" se usa solo en la capa (layer) de entrada\n",
    "\n",
    "    #¿Que recibe de entrada y que obtiene de salida estas capas?\n",
    "    #La entrada debe ser un tensor de 4D con el formato:\n",
    "    #   (batch_size, rows, cols, channels), si data_format='channels_last' esta \n",
    "    #     es la opcion por defecto de \"data_format\"\n",
    "    #\n",
    "    #   (batch_size, channels, rows, cols), si data_format='channels_first'\n",
    "    #\n",
    "    #La salida es otro tensor de 4D con el formato:\n",
    "    #   (batch_size, new_rows, new_cols, filters), si data_format='channels_last' \n",
    "    #     esta es la opcion por defecto de \"data_format\"\n",
    "    #\n",
    "    #   (batch_size, filters, new_rows, new_cols), si data_format='channels_first' \n",
    "    #\n",
    "    #rows and cols pueden variar dependiendo de la bandera \"padding\", con \"same\"\n",
    "    #se tendrían la misma candtidad de rows y cols de salida que de entrada.\n",
    "\n",
    "    #Primera capa convolucional\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same', input_shape=img_shape))\n",
    "\n",
    "    #Se agrega una funcion de activación, se puede agregar directamente en la capa\n",
    "    #convolucional pero esto haría que se pusiera la activación en la entrada, \n",
    "    #se decide poner la activación en la salida.\n",
    "    #La función de activación que se usa es la \"relu\" pero existen otras funciones\n",
    "    #de activación, estas se pueden encontrar aqui:\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/keras/activations \n",
    "    #Relu hace que los valores con un valor menor a 1 se vuelvan 0\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #Como resultado de la primera capa convolucional se tiene 32 capas de profundidad\n",
    "    #y cada una de estas capas poseen las mismas dimensiones que la imagen de entrada\n",
    "    #en caso de mantener la configuracion por defecto se tiene 64x64x32, se puede \n",
    "    #ver como un cubo con tres dimensiones: largo, ancho y profundidad \n",
    "\n",
    "    #Segunda capa convolucional, con 32 filtros\n",
    "    #Esta segunda capa sigue siendo convolucional pero notese que no posee el \n",
    "    #padding, como resultado se pierde las últimas filas y columnas (los bordes)\n",
    "    #por lo que para este caso el cubo seria de dimenciones 62x62x32, ya que \n",
    "    #aún se tienen los 32 filtros. La función de activación sigue siendo \"relu\".\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    #Una capa Maxpolling2D reduce la cantidad de col y rows, en este caso\n",
    "    #se reduce de una matriz 2x2 a un único escalar, como consecuencia se tiene\n",
    "    #como resultado un cubo 31x31x32.\n",
    "    #Los layers de pool lo que hacen es reducir la cantidad de filas y columnas\n",
    "    #de entrada, esto puede hacerse por promedio, minimo, maximo o una función\n",
    "    #en este caso MaxPooling lo que hace es usar el valor máximo de la matriz que\n",
    "    #está viendo. (de la matriz 2x2 agarra el valor máximo y ese es el resultado)\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #Las capas dropout se usa para evitar el overfiting, estas solo se activan\n",
    "    #durante el entrenamiento, y su función es hacer cero entradas de manera aleatoria\n",
    "    #con un taza de ocurrencia seleccionada por el usuario (en este caso es de 25%)\n",
    "    #las que no son colocadas en 0 se multiplican por 1/(1-rate)\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #Tercera capa convolucional, con 64 filtros\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    #Cuarta capa convolucional, con 64 filtros\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #La capa \"flatten\" lo que hace es pasar de las capas convolucionales y 3D a \n",
    "    #capas en una dimensión. Un único vector horizontal.\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Se reduce la cantidad de neuronas, en este casi particular se pasa de la cantidad\n",
    "    #dada por Flatten y se pasa a 512 nueronas, se recomienda siempre usar 2^n valores\n",
    "    #Esta capa se le conoce como fully connected layer\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #Finalmente se reduce a la cantidad de neuronas equivalentes a la cantidad de clases\n",
    "    #que se desean, en nuestro caso, la cantidad de personajes que estamos clasificando\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    r, c = 2, 2\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow((gen_imgs[cnt] * 255).astype(int))\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"ResultadosCafe/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # Load the dataset\n",
    "    X_train, y = get_dataset(save=True)\n",
    "    \n",
    "    #X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Se seleccionan imagenes aleatorias para hacer la epoca\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # El generador da un numero de imagenes sinteticas para la epoca\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        #Se entrena con las imagenes reales\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        \n",
    "        #Se entrena con las imagenes creadas como falsas\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Se ingresa el ruido al sistema combinado, la idea es que\n",
    "        # el generador engañe al discriminador para que piense que es\n",
    "        # una imagen real\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # Se imprime el progreso\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % save_interval == 0:\n",
    "            print (\"%d Discriminator_loss: %f acc.: %.2f%% Generator_loss: %f\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=80000, batch_size=16, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "generator.save('generator_7_30_2020.h5')\n",
    "discriminator.save_weights('discriminator_7_30_2020_weigths.h5')\n",
    "combined.save_weights('combined_7_30_2020_weigths.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "path=\"images/mnist_0.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "path=\"images/mnist_80000.png\"\n",
    "display(Image.open(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
